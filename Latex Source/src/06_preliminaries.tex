\chapter{Preliminaries}
This chapter provides essential background information and definitions that form the foundation for the subsequent discussions in the document. Firsty, in \secref{sec:general_definitions}, a plethora of useful general defitions are provided in order to facilitate the subsequent sections and, as a resul, the rest of the document as well. 
\ref{sec:mpc_theory} introduces the concept of Model Predictive Control (MPC) as an advanced control strategy for dynamic systems, particularly focusing on the linear case. The linear MPC problem is formulated with a finite prediction horizon, system dynamics, and constraints on state and control variables. Additionally, the stability of MPC systems is briefly discussed, emphasizing the importance of demonstrating recursive feasibility and establishing stability in the sense of Lyapunov. 

\section{General Definitions}\label{sec:general_definitions}
\begin{definition}{Positive Semidefinite}
	A positive semidefinite matrix is a concept in linear algebra and matrix theory. A symmetric matrix \(A\) is said to be positive semidefinite if, for any non-zero column vector \(x\), the following inequality holds:
	\begin{equation}
		x^T Ax \geq 0
	\end{equation}
	Here, \(x^T\) represents the transpose of the vector \(x\), and \(x^T Ax\) is the quadratic form associated with the matrix \(A\). Another way to express positive semidefiniteness is in terms of eigenvalues. A symmetric matrix \(A\) is positive semidefinite if and only if all of its eigenvalues are non-negative.\\
	Mathematically, $A \succeq 0 $ indicates positive semidefiniteness.
\end{definition}\\



\begin{definition}{Positive Definite}
	A positive definite matrix is a more specific case of a positive semidefinite matrix. A symmetric matrix \(A\) is said to be positive definite if, for any non-zero column vector \(x\), the following inequality holds:
	\begin{equation}
		x^T Ax > 0
	\end{equation}
	Here, \(x^T\) represents the transpose of the vector \(x\), and \(x^T Ax\) is the quadratic form associated with the matrix \(A\). Alternatively, positive definiteness in terms of eigenvalues is expressed as follows: A symmetric matrix \(A\) is positive definite if and only if all of its eigenvalues are strictly positive.
	In mathematical notation, \(A \succ 0\) indicates positive definiteness.
\end{definition}\\



\begin{definition}{Half-Space}
	A half-space in \(n\)-dimensional space is defined by a linear inequality. The general form of a half-space is given by:
	\begin{equation}
		H = \{x \in \mathbb{R}^n \mid a_1x_1 + a_2x_2 + \ldots + a_nx_n \leq b\}
	\end{equation}
	Alternatively, in vector form:
	\begin{equation}
		H = \{x \in \mathbb{R}^n \mid a^Tx \leq b\}
	\end{equation}
	Here, \(a_1, a_2, \ldots, a_n\) are real constants, and \(b\) is a real constant. The vector \((a_1, a_2, \ldots, a_n)\) is the normal vector to the hyperplane defining the boundary of the half-space. The inequality \(a_1x_1 + a_2x_2 + \ldots + a_nx_n \leq b\) represents the side of the hyperplane where the half-space lies.
	Geometrically, a half-space is one of the two regions divided by a hyperplane. If the inequality is strict (\(<\) instead of \(\leq\)), the half-space does not include points on the hyperplane itself. If the inequality is non-strict (\(\leq\)), the half-space includes points on the hyperplane. \\
	In two dimensions (\(n = 2\)), a half-space is a region divided by a straight line. In three dimensions (\(n = 3\)), it is a region divided by a plane, and so on. The intersection of multiple half-spaces forms a polyhedral region.
\end{definition}\\



\begin{definition}{Polyhedral Region}
	A polyhedral region is a geometric object in three-dimensional space that is defined as the intersection of a finite number of half-spaces. 
	Formally, a polyhedral region \(P\) in three-dimensional space can be expressed as:
	\[ P = \{ \mathbf{x} \in \mathbb{R}^3 \mid \mathbf{a}_i \cdot \mathbf{x} \leq b_i, \quad i = 1, 2, \ldots, n \} \]
	where \(\mathbf{x}\) is a three-dimensional vector representing a point in space, \(\mathbf{a}_i\) are vectors normal to the defining planes, and \(b_i\) are constants determining the position of these planes. The inequalities \(\mathbf{a}_i \cdot \mathbf{x} \leq b_i\) specify that the point \(\mathbf{x}\) lies on or inside the half-space defined by the corresponding plane.
\end{definition}\\

\begin{definition}{Recursive feasibility}
	The MPC problem is deemed recursively feasible if, for all feasible initial states, assurance of feasibility is maintained at every state along the closed-loop trajectory.
\end{definition}\\

\begin{definition}{Positively Invariant Set}
	A set $\Omega$ is said to be a positively invariant set for a system $x(k+1) = f_\kappa (x(k))$, if
	\begin{equation*}
		x(k)\in \Omega \implies x(k+1) \in \Omega \quad \quad \forall k \in \mathbb{N}
	\end{equation*}
\end{definition}\\



\begin{definition}{Comparision Functions}\\
	For $\mathbb{R}_0^+ [0,\infty)$, the following comparison functions exist
	\begin{equation}
		\mathcal{K} := \left\{
		\begin{aligned}
			\alpha : \mathbb{R}_0^+ \rightarrow \mathbb{R}_0^+\Bigg|\alpha \text{ is continuous}\text{ and strictly increasing with }\alpha(0) = 0
		\end{aligned}
		\right\}
	\end{equation}\\
	\begin{equation}
		\mathcal{K}_\infty := \left\{
		\begin{aligned}
			\alpha : \mathbb{R}_0^+ \rightarrow \mathbb{R}_0^+\Bigg|\alpha \in \mathcal{K} \text{ and unbounded}
		\end{aligned}
		\right\}
	\end{equation}\\
	\begin{equation}
		\mathcal{K}\mathcal{L}:= \left\{
		\begin{aligned}
			&\beta \text{ continous}\\
			\beta :  \mathbb{R}_0^+\times\mathbb{R}_0^+ \rightarrow \mathbb{R}_0^+\Bigg|&\beta(\cdot, t) \in \mathcal{K} \quad \forall t \in  \mathbb{R}_0^+\\
			&\beta(r,\cdot) \text{ is strictly decreasing to 0 }\forall r\in  \mathbb{R}_0^+ 
		\end{aligned}
		\right\}
	\end{equation}\\
\end{definition}

\begin{definition}{Lyapunov Function}\\
	Let $Y \subseteq X$ be a forward invariant set and $x^* \in X$. A function $V : Y \rightarrow \mathbb{R}^+$ is called a Lyapunov function for $x^+ = g(x)$ if the following two conditions hold for all $x \in Y$:
	
	\begin{enumerate}
		\item There exist $\alpha_1, \alpha_2 \in \mathcal{K}_\infty$ such that
		\[
		\alpha_1(\|x - x^*\|) \leq V(x) \leq \alpha_2(\|x - x^*\|).
		\]
		
		\item There exists $\alpha_V \in \mathcal{K}$ such that
		\[
		V(x^+) \leq V(x) - \alpha_V(\|x - x^*\|).
		\]
	\end{enumerate}
\end{definition}



\begin{definition}{Lyapunov Function}. \\
	Let $\mathcal{X}$ be a positively invariant set for a system $x(k+1) = f_\kappa (x(k))$ containing a neighbour of the origin in its interior. A function $V : \mathcal{X} \rightarrow R_+$ is called a Lyapunov function in $\mathcal{X}$ if for all $x \in \mathcal{X}$:
	\begin{align*}
		&V(x) > 0 \quad \forall x \neq 0\\
		&V(x) = 0 \quad\\
		&V(x(k+1)) - V(x(k)) \leq -\alpha(x) \quad\\
		%&V(x(k+1)) - V(x(k)) +\alpha(x)\leq 0 \quad\\
	\end{align*}
	
	
\end{definition}




\section{Model Predictive Control}\label{sec:mpc_theory}

Model Predictive Control (MPC) is an advanced control strategy used for dynamic systems. In the case of a linear time-invariant system, the mathematical description of MPC involves optimizing a cost function over a finite prediction horizon while subject to system dynamics and constraints. \\
For the purpose of this work, we will only consider the linear case of the MPC. For more details, please refer to \cite{rawlings2020model}. \\
Consider a linear system described by the following state-space equations:
\begin{align}
	&x_{k+1} = Ax_k + Bu_k \nonumber \\
	 &y_k = Cx_k \label{eq:linear_system}
\end{align}
where:
\begin{itemize}
	\item $x_k$ is the state vector at time $k$
	\item $u_k$ is the control input (or control variable) at time $k$
	\item $y_k$ is the output at time $k$
	\item $A$, $B$, and $C$ are matrices that define the system dynamics.
\end{itemize}
The objective is to minimize a cost function $J$ over a finite prediction horizon \(N\). The cost function is typically defined as the sum of a quadratic performance index over the prediction horizon. \\
The choice of cost function is critical and highly depends on the final objective. For example, in case the objective is to track a reference signal, a typical cost function assumes the form described in \equaref{eq:cost_f_1}. 
\begin{equation}
	J = \sum_{i=0}^{N-1} \left[ (y_{k+i} - r_{k+i})^T Q (y_{k+i} - r_{k+i}) + u_{k+i}^T R u_{k+i} \right] 
	\label{eq:cost_f_1}
\end{equation} 
where
\begin{itemize}
	\item \(r_{k+i}\) is the reference trajectory at time \(k+i\)
	\item  $Q \succeq 0$ is the weighting matrix for the state error
	\item $R \succ 0$ is the weighting matrix for the control input
\end{itemize}
Alternatively, another typical cost function has the followin form, 

\begin{equation}
	J = x^T_{N}Px_{N} + \sum_{i=0}^{N-1} x^T_{i}Qx_{i} + u^T_{i}Ru_{i}
	\label{eq:cost_f_2}
\end{equation} 
where
\begin{itemize}
	\item $x_i$ is the state vector at time $i$
	\item $u_i$ is the control variable at time $i$
	\item  $Q \succeq 0$ is the weighting matrix for the state error
	\item  $P \succeq 0$ is the weighting matrix indicating the terminal cost
	\item $R \succ 0$ is the weighting matrix for the control input.
\end{itemize}

In \equaref{eq:cost_f_2}, the term $x^T_{N}Px_{N}$ is used to mitigate the fact that the MPC is minimizing over a limted time-horizon by ensuring that the system converges to a desired state by the end of the prediction horizon. While the infinite horizon formulation, i.e. $N = \infty$ and $P=0$, possesses nice properties, such as its robustness and the perfect tradeoff between short- and long-term benefits of actions, it can not be computed in practice. Intuitively, since MPC is dealing with optimization problems under constraints, an infinite horizon will bring to having an infinte amount of variables. This is therefore the motivation behind why the horizon is usually shortened to $N$ steps and the terminal cost is introduced. \\
The Linear MPC problem, therefore, is formulated as follows. \\
\begin{equation}
	\begin{aligned}
		J^* = \text{\textbf{min} }&x^T_{N}Px_{N} + \sum_{i=0}^{N-1} x^T_{i}Qx_{i} + u^T_{i}Ru_{i}\\
		\text{\textbf{s.t.}	}&x_{k+i+1} = Ax_{k+i} + Bu_{k+i} \quad \text{for } i = 0, 1, \ldots, N-1 \\
		 &x_{k+i} \in \mathcal{X}, u_{k+i} \in \mathcal{U} \quad \text{for } i = 0, 1, \ldots, N-1\\
		 &x_{k+N} \in \mathcal{X}_f\\
		 &x_{k} = x(k)
	\end{aligned}
\end{equation}
where $J^*$ is the global minimum of $J$ and $\mathcal{X}, \mathcal{X}_f,\mathcal{U}$ are polyhedral regions. These are used to indicate the constraints over the state and control variables, which have the following form:
\begin{align*}
	x_{\text{min}} &\leq x_{k+i} \leq x_{\text{max}} \quad \text{for } i = 0, 1, \ldots, N \\
	x_{\text{min}|f} &\leq x_{k+i} \leq x_{\text{max}|f} \quad \text{for } i = 0, 1, \ldots, N \\
	u_{\text{min}} &\leq u_{k+i} \leq u_{\text{max}} \quad \text{for } i = 0, 1, \ldots, N-1 
\end{align*}
Furthermore, the optimization problem is subject to system dynamics:
\begin{align*}
	x_{k+i+1} &= Ax_{k+i} + Bu_{k+i} \quad \text{for } i = 0, 1, \ldots, N-1 \\
\end{align*}

The solution to this optimization problem provides the optimal control sequence \(u_k^*, u_{k+1}^*, \ldots, u_{k+N-1}^*\). The first element of this sequence, \(u_k^*\), is then applied to the system, and the optimization problem is solved again at the next time step.
\subsection{Stability of MPC systems}\label{sec:stable_mpc_systems}
While there exist multiple ways to prove stability of MPC systems, this section will only focus on the one which is mostly relevant for this work, namely proving stability by leveraging the characteristics of the terminal set. More specifically, by defining $\mathcal{X}_f$ as convex set which includes the origin ($x(N) = 0$), one can prove stability by insuring certain properties to be satisfied. These properties assume the cost function to have the following form. 
\begin{equation}
	J(x) = min_{x, u} V_f(X(N)) + \sum_{t =0}^{N-1}I(x(t), u(t))
\end{equation}
where $I(x(t), u(t))$ is the stage cost and $V_f(X(N))$ is the terminal cost. \\
Accordingly, these are the properties to satisfy.
\begin{enumerate}
	\item The stage cost must be strictly positive and zero at the origin.
	\item The terminal set is invariant under the local control law $\kappa_f(x)$. Namely 
	\begin{equation}
		x(t+1) = Ax + B\kappa_f(x)\in\mathcal{X}_f \quad \forall x\in \mathcal{X}_f
	\end{equation}
	given that $X_f \subseteq X$ and $\kappa_f(x) \in \mathcal{U}$
	 
	\item Establish stability by illustrating that the terminal cost function serves as a Lyapunov function in $\mathcal{X}_f$. 
	\begin{equation}
		V_f(x(t+1)) - V_f(x) \leq -I(x, \kappa_f)\quad \forall x \in \mathcal{X}_f
	\end{equation}
\end{enumerate}


\section{Graph Transformation Theory}
Graph Transformation Theory (GTT),or graph rewriting, is a mathematical framework used to model and analyze the transformation of graphs. Graphs in this context represent structures composed of nodes and edges, and the transformations involve changing the structure of these graphs.\\
This section focuses on the fundamentals of this discipline and for more information, the reader is invited to consult the following resources: \cite{HECKEL2006187}, \cite{handbook_of_gg}, \cite{essential_on_graph_theory}, \cite{2123-19534} and \cite{gtt_se}. The notions for this section have been collected while studying the aforenamed resources. \\
In order to introduce GTT, some concepts of category theory are necessary. \\

\begin{definition}{Category}\\
A category $\mathcal{C}$ consists of the following data:
\begin{enumerate}
	\item Objects : A group of mathematical objects, denoted as $\text{Obj}(\mathcal{C})$
	\item Morphisms (Arrows/Maps): For every pair of objects \(A\) and \(B\), there is a set of morphisms denoted as \(\text{Hom}_{\mathcal{C}}(A, B)\). If \(f \in \text{Hom}_{\mathcal{C}}(A, B)\), we write \(f: A \to B\). Morphisms must satisfy two properties:
	\begin{itemize}
		\item  Associativity: For every triple of objects \(A, B, C\) and morphisms \(f: A \to B\), \(g: B \to C\), and \(h: C \to D\), the composition \((h \circ g) \circ f\) is the same as \(h \circ (g \circ f)\).
		\item Identity : For every object \(A\), there exists an identity morphism \(1_A: A \to A\) such that \(f \circ 1_A = f\) and \(1_B \circ f = f\) for any morphism \(f: A \to B\).
	\end{itemize}
	\item Composition: For each pair of morphisms \(f: A \to B\) and \(g: B \to C\), there exists a composite morphism \(g \circ f: A \to C\) in \(\text{Hom}_{\mathcal{C}}(A, C)\). Composition must be associative, i.e., \((h \circ g) \circ f = h \circ (g \circ f)\).
\end{enumerate}
\end{definition}




\begin{example}{Category of graphs}
	A graph can be represented as the category having
	\begin{enumerate}
		\item Objects : tuple $G = (\mathcal{V},\mathcal{E},s,t)$ where
		\begin{itemize}
			\item $\mathcal{V}$ is the set of nodes
			\item $\mathcal{E}$ is the set of edges connecting the nodes
			\item $s, t : \mathcal{E}\rightarrow\mathcal{V}$ are the source (target) of the edges, referred to as source (target) function
		\end{itemize}
		\item Morphisms: $f : G \rightarrow H$ must respect source and target functions, ie:
		\begin{align*}
			\forall e \in \mathcal{E}.f(s(e)) &= s(f(e))\\
			\forall e \in \mathcal{E}.f(t(e)) &= t(f(e))\\
		\end{align*}
	\end{enumerate}
\end{example}


\begin{definition}{Product in a Category}\\
	Let $\mathcal{C}$ be a category. Let $X_1$ and $X_2$ be objects of $\mathcal{C}$. A product of $X_1$ and $X_2$ is an object $X$, typically denoted $X_1 \times X_2$, equipped with a pair of morphisms $\pi_1: X \to X_1$ and $\pi_2: X \to X_2$ satisfying the following universal property:
	
	For every object $Y$ and every pair of morphisms $f_1: Y \to X_1$ and $f_2: Y \to X_2$, there exists a unique morphism $f: Y \to X_1 \times X_2$ such that the following diagram commutes:
	
	\begin{center}
		\begin{tikzcd}
			& Y \arrow[ld, "f_1"'] \arrow[rd, "f_2"] \arrow[d, "f", dashed] & \\
			X_1 & X_1 \times X_2 \arrow[l, "\pi_1"'] \arrow[r, "\pi_2"] & X_2
		\end{tikzcd}
	\end{center}
\end{definition}


\begin{definition}{Coproduct in a Category}\\
Let $\mathcal{C}$ be a category. Consider objects $X_1$ and $X_2$ in $\mathcal{C}$. A coproduct of $X_1$ and $X_2$ is an object $X$, often denoted as $X_1 \sqcup X_2$, equipped with a pair of morphisms $i_1: X_1 \to X_1 \sqcup X_2$ and $i_2: X_2 \to X_1 \sqcup X_2$ satisfying the following universal property:

For every object $Y$ and every pair of morphisms $f_1: X_1 \to Y$ and $f_2: X_2 \to Y$, there exists a unique morphism $f: X_1 \sqcup X_2 \to Y$ making the following diagram commute:

\[
\begin{tikzcd}
	X_1 \arrow[r, "i_1"] \arrow[rd, "f_1"'] & X_1 \sqcup X_2 \arrow[d, "f", dashed] & X_2 \arrow[l, "i_2"'] \arrow[ld, "f_2"] \\
	& Y &
\end{tikzcd}
\]

This means that any other morphism $X_1 \sqcup X_2 \to Y$ that respects $i_1$ and $i_2$ factors uniquely through the coproduct $X_1 \sqcup X_2$.
In other words, the coproduct of $X_1$ and $X_2$ is an object $X_1 \sqcup X_2$ along with morphisms $i_1$ and $i_2$ such that, for any other object $Y$ and morphisms $f_1$ and $f_2$, there exists a unique morphism $f$ making the diagram commute. The coproduct captures the idea of combining objects $X_1$ and $X_2$ in a way that respects morphisms to another object $Y$.
\end{definition}
\\ 

\begin{definition}{Pushout}\\
Given a category with three objects \(A\), \(B\), and \(C\), and two morphisms (arrows) \(f: A \to B\) and \(g: A \to C\), the pushout of \(f\) and \(g\) is an object \(P\) along with two morphisms \(i_B: B \to P\) and \(i_C: C \to P\) such that the following diagram 
\begin{center}
	\begin{tikzcd}
		A \arrow[r, "f"] \arrow[d, "g"'] & B \arrow[d, "i_B"] \\
		C \arrow[r, "i_C"'] & P
	\end{tikzcd}
\end{center}
commutes and such that $(P, i_1, i_2)$ is universal. That is, for any other such triple $(Q, j_1, j_2)$ for which the following diagram commutes, there must exist a unique u : P → Q also making the diagram commute:
\begin{center}
	\begin{tikzcd}
		A \arrow[r, "f"] \arrow[d, "g"'] & B \arrow[d, "i_B"] \arrow[rdd, "j_2", bend left]\\
		C \arrow[r, "i_C"'] \arrow[rrd, "j_1", bend right] & P\arrow[rd, "u", dashed]\\
		&&Q
	\end{tikzcd}
\end{center}
In simpler terms, the pushout is a construction in category theory that combines two morphisms (arrows) \(f: A \to B\) and \(g: A \to C\) in a way that respects the existing structure of the category.

Given two objects \(B\) and \(C\) with morphisms \(f\) and \(g\) from a common object \(A\), the pushout \(P\) is another object, along with two morphisms \(i_B: B \to P\) and \(i_C: C \to P\). These morphisms create a commutative diagram, meaning that following any path from \(A\) to \(P\) results in the same composition, regardless of the route taken.

Moreover, the pushout has a universal property, stating that for any other object \(Q\) with morphisms \(j_1\) and \(j_2\) forming a similar commutative diagram, there exists a unique morphism \(u: P \to Q\) making the entire diagram commute.

In simpler terms, the pushout captures a way of amalgamating information from \(B\) and \(C\) in a manner that is compatible with the given morphisms, and it does so in a universal way that any alternative attempt to amalgamate would factor uniquely through the pushout.
\end{definition}\\

\begin{definition}{The Double Pushout Approach (DPO)}\\
	In the DPO, productions have the following form:
	\begin{center}
		\begin{tikzcd}
			p:L&K  \arrow[l, "l"'] \arrow[r, "r"] & R
		\end{tikzcd}
	\end{center}
	where:
	\begin{itemize}
	\item $L$ (Left-hand side (LHS)) is a graph representing the initial pattern or structure to be recognized in the larger graph. This is the part of the graph that the production rule aims to match and replace.
	\item $K$ (Interface graph) represents the region where the left-hand side $L$ is embedded in the larger graph.
	\item $R$ (Right-hand side (RHS)) is a graph representing the replacement or transformation that will be applied to the matched portion of the graph. After the application of the production rule, the matched portion of the graph (given by \(L\) and \(K\)) will be replaced by the structure defined in \(R\).
	\item \(l: K \rightarrow L\) is a morphism representing the embedding of the interface graph \(K\) into the left-hand side \(L\).
	\item \(r: K \rightarrow R\) is a morphism representing the embedding of the interface graph \(K\) into the right-hand side \(R\).
	\end{itemize}

	The morphisms are usually injective, indicating that each element in the interface graph \(K\) has a unique counterpart in \(L\) and \(R\), respectively.
	The overall interpretation of \(p: L \leftarrow K \rightarrow R\) is that when the left-hand side \(L\) and its context \(K\) are found in a larger graph, they can be replaced by the right-hand side \(R\) according to the specified morphisms \(l\) and \(r\).  \\
	This production is applied as follows.
	\begin{center}
		\begin{tikzcd}
			L\arrow[d, "m"]&K  \arrow[l, "l"'] \arrow[r, "r"] \arrow[d, "d"] & R\arrow[d, "m'"] \\
			G&D  \arrow[l, "l'"'] \arrow[r, "r'"] & H
		\end{tikzcd}
	\end{center}
	
	So, when this production rule is applied to \(G\), the subgraph \(D\) matching the left-hand side \(L\) is replaced by the right-hand side \(R\), resulting in the transformed graph \(H\). The morphisms \(m\), \(d\), and \(m'\) ensure that the embedding and replacement are consistent with the larger graphs. The commutativity of the diagram ensures that the rewriting process is well-defined and respects the structure of the graphs involved.
	
	\begin{enumerate}
		\item Matching and Interface: The interface graph $K$ is matched within the original graph $G$ using the matching morphism $d: K \to D$. This identifies where the LHS $L$ is located in $G$.
		
		\item LHS Embedding: The LHS $L$ is embedded into the original graph $G$ using the embedding morphism $m: L \to G$. This creates a graph that includes the LHS structure.
		
		\item Application of Production Rule: The LHS $L$ is then replaced by the RHS $R$, respecting the embedding morphisms $l$ and $r$. This transformation is performed within the identified context given by the interface graph.
		
		\item Resulting Graph: The resulting graph $H$ is obtained after the replacement. The context graph $D$ is embedded into $H$ using $r': D \to H$, and the RHS $R$ is included in $H$ using $m': R \to H$.
	\end{enumerate}
\end{definition}

\begin{example}
	Let's define a graph that represents a simple directed network:
	\begin{equation}
		G = \{ \text{Nodes: } A, B, C, D; \text{ Edges: } (A \rightarrow B), (B \rightarrow C), (C \rightarrow D) \}
	\end{equation}
	Let's define a DPO production rule to replace a chain of three nodes with a new node. The production rule is as follows:
	\begin{equation}
		p: L \leftarrow K \rightarrow R
	\end{equation}
	Where:
	\begin{itemize}
		\item\(L\) (LHS): Three consecutive nodes forming a chain - \(A \rightarrow B \rightarrow C\).
		\item\(K\) (Interface graph): The interface graph is the same chain of nodes - \(A \rightarrow B \rightarrow C\).
		\item\(R\) (RHS): A single node - \(X\).
		\item\(l: K \rightarrow L\) is the identity morphism since \(L\) and \(K\) are the same chain.
		\item\(r: K \rightarrow R\) maps the chain of nodes to the single replacement node.
	\end{itemize}

	The production rule essentially says that if a chain of three nodes is found in a graph, it can be replaced with a single node. The interface graph \(K\) ensures that the replacement occurs in the correct context.
	
	\begin{align*}
		\text{Initial Graph:} & \quad G = \{A \rightarrow B \rightarrow C \rightarrow D\} \\
		\text{Production Rule:} & \quad p:  \underset{L}{\underbrace{A \rightarrow B \rightarrow C}}\rightarrow\underset{K}{\underbrace{A \rightarrow B \rightarrow C}} \rightarrow \underset{R}{\underbrace{X}} \\
		\text{Resulting Graph:} & \quad H =  \{X \rightarrow D\}
	\end{align*}
	After applying the production rule, the chain \(A \rightarrow B \rightarrow C\) has been replaced by a single node \(X\), resulting in the transformed graph \(H\). 
\end{example}\\


\begin{definition}{Dangling Condition \\}
	Let \(G = (\mathcal{V}, \mathcal{E})\) be a graph with vertices $\mathcal{V}$ and edges $\mathcal{E}$. The dangling condition imposes that, for each edge \(e_i = (v_s, v_t) \in \mathcal{E}\), if \(e_i\) is not deleted, then both its source node \(v_s\) and target node \(v_t\) should be preserved. This condition is an essential condition in the DPO framework.
\end{definition}\\

\begin{definition}{Identification Condition \\}
	Let \(G = (\mathcal{V}_G, \mathcal{E}_G)\) be a graph and \(H = (\mathcal{V}_H, \mathcal{E}_H)\) be a host graph. Suppose there exists a morphism \(f: \mathcal{V}_G \rightarrow \mathcal{V}_H\) and \(g: \mathcal{E}_G \rightarrow \mathcal{E}_H\) such that two nodes or edges in \(G\), denoted as \(v_{1}, v_{2} \in \mathcal{V}_G\) or \(e_{1}, e_{2} \in \mathcal{E}_G\), are matched into a single node or edge in \(H\) (i.e., \(f(v_{1}) = f(v_{2})\) or \(g(e_{1}) = g(e_{2})\) via a non-injective morphism), the identification condition imposes that those should be preserved. This condition is an essential condition in the DPO framework.
\end{definition}


